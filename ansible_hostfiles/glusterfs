# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes
etcd
glusterfs

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user=root

# If ansible_ssh_user is not root, ansible_sudo must be set to true
#ansible_sudo=true
#ansible_become=yes

# Install Enterprise or Origin; set up ntp
openshift_deployment_type=openshift-enterprise
openshift_clock_enabled=true

# Network/DNS Related
openshift_master_default_subdomain=apps.172.16.1.10.nip.io
osm_cluster_network_cidr=10.1.0.0/16
osm_host_subnet_length=8
openshift_portal_net=172.30.0.0/16
osm_default_node_selector="region=primary"
openshift_docker_insecure_registries=172.30.0.0/16

# CNS Storage
openshift_storage_glusterfs_namespace=glusterfs
openshift_storage_glusterfs_name=storage
openshift_storage_glusterfs_heketi_wipe=True

# Automatically Deploy the router and registry
openshift_hosted_manage_router=true
openshift_hosted_manage_registry=true
openshift_hosted_registry_storage_kind=glusterfs
openshift_hosted_registry_storage_volume_size=25Gi
openshift_hosted_router_selector='region=infra'
openshift_hosted_registry_selector='region=infra'

# Disable Checks and Broker
# ansible_service_broker_install=false
# template_service_broker_install=false
openshift_disable_check=disk_availability,docker_storage,memory_availability

## Uncomment for multitenant functionality (i.e. each project gets it's own "private" network)
#os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'

# Metrics and Logging doesn't install well with CNS. Do it as a POST install step. See [POST INSTALL] for 
# notes on what to do after OCP and CNS are deployed with the ansible installer

# If using Route53 or you're pointed to the master with a "vanity" name
openshift_master_public_api_url=https://ocp.172.16.1.10.nip.io:8443
openshift_master_public_console_url=https://ocp.172.16.1.10.nip.io:8443/console
openshift_master_cluster_public_hostname=ocp.172.16.1.10.nip.io

# The following enabled htpasswd authentication
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/openshift-passwd'}]
openshift_master_htpasswd_users={'developer': '$apr1$q2fVVf46$85HP/4JHGYeFBKAKPBblo0'}

# Disable Checks and Broker
# ansible_service_broker_install=false 
openshift_disable_check=disk_availability,docker_storage,memory_availability

# host group for etcd
[etcd]
master.172.16.1.10.nip.io

# host group for masters
[masters]
master.172.16.1.10.nip.io openshift_schedulable=true

# host group for nodes, includes region info
[nodes]
master.172.16.1.10.nip.io openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
node1.172.16.1.11.nip.io openshift_node_labels="{'region': 'primary', 'zone': 'z1'}"
node2.172.16.1.12.nip.io openshift_node_labels="{'region': 'primary', 'zone': 'z2'}"
node3.172.16.1.13.nip.io openshift_node_labels="{'region': 'primary', 'zone': 'z2'}"

[glusterfs]
node1.172.16.1.11.nip.io glusterfs_ip=172.16.1.11 glusterfs_devices='[ "/dev/vdb" ]'
node2.172.16.1.12.nip.io glusterfs_ip=172.16.1.12 glusterfs_devices='[ "/dev/vdb" ]'
node3.172.16.1.13.nip.io glusterfs_ip=172.16.1.13 glusterfs_devices='[ "/dev/vdb" ]'
##
##
### [POST INSTALL]
### Run the following comand to set gluster as the default storage provider
###           oc annotate storageclass glusterfs storageclass.beta.kubernetes.io/is-default-class="true"
###
### Check to see if the storageClass is set to default
###          oc get storageClass
###
### Next, run the metrics and logging installers
###
###          ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-metrics.yml \
###          -e openshift_metrics_install_metrics=True \
###          -e openshift_metrics_hawkular_hostname=hawkular.apps.172.16.1.10.nip.io \
###          -e openshift_metrics_cassandra_storage_type=dynamic \
###          -e openshift_metrics_cassandra_pvc_size=20Gi
###
###          ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-logging.yml \
###          -e openshift_logging_kibana_hostname=kibana.apps.172.16.1.10.nip.io \
###          -e openshift_logging_install_logging=true \
###          -e openshift_logging_es_pvc_dynamic=true \
###          -e openshift_logging_es_pvc_size=20Gi

