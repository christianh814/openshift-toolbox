# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes
etcd
glusterfs

# Set variables common for all OSEv3 hosts
[OSEv3:vars]

# If ansible_ssh_user is not root, ansible_sudo must be set to true
ansible_ssh_user=root

# Install Enterprise or Origin; set up ntp
openshift_deployment_type=openshift-enterprise
openshift_clock_enabled=true

# Network/DNS Related
openshift_master_default_subdomain=apps-poc.cloud.chx
osm_cluster_network_cidr=10.1.0.0/16
osm_host_subnet_length=8
openshift_portal_net=172.30.0.0/16
osm_default_node_selector="region=primary"
openshift_docker_insecure_registries=172.30.0.0/16
# container_runtime_docker_storage_setup_device=/dev/nvme1n1

# CNS Storage
openshift_storage_glusterfs_namespace=glusterfs
openshift_storage_glusterfs_name=storage
openshift_storage_glusterfs_heketi_wipe=true
openshift_storage_glusterfs_wipe=true
openshift_storage_glusterfs_storageclass_default=true
openshift_storage_glusterfs_block_storageclass=true
openshift_storage_glusterfs_block_host_vol_size=50

# Automatically Deploy the router
openshift_hosted_manage_router=true
openshift_hosted_router_selector='region=infra'

# Automatically deploy the registry using glusterfs
openshift_hosted_manage_registry=true
openshift_hosted_registry_storage_kind=glusterfs
openshift_hosted_registry_storage_volume_size=25Gi
openshift_hosted_registry_selector='region=infra'

# Disble Checks
openshift_disable_check=disk_availability,docker_storage,memory_availability

# Mulititenant functionality (i.e. each project gets it's own "private" network)
os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'

# Uncomment when setting up logging/metrics/prometheus
openshift_master_dynamic_provisioning_enabled=true
dynamic_volumes_check=False

# Logging
openshift_logging_install_logging=true
openshift_logging_es_pvc_dynamic=true
openshift_logging_es_pvc_size=28Gi
openshift_logging_es_pvc_storage_class_name=glusterfs-storage-block
openshift_logging_curator_nodeselector={'region':'infra'}
openshift_logging_es_nodeselector={'region':'infra'}
openshift_logging_kibana_nodeselector={'region':'infra'}
openshift_logging_es_memory_limit=4G

# Metrics
openshift_metrics_install_metrics=true
openshift_metrics_cassandra_storage_type=dynamic
openshift_metrics_cassandra_pvc_size=28Gi
openshift_metrics_cassanda_pvc_storage_class_name=glusterfs-storage-block
openshift_metrics_hawkular_nodeselector={'region':'infra'}
openshift_metrics_heapster_nodeselector={'region':'infra'}
openshift_metrics_cassandra_nodeselector={'region':'infra'}

## Prometheus Metrics
openshift_hosted_prometheus_deploy=true
openshift_prometheus_namespace=openshift-metrics
openshift_prometheus_node_selector={'region':'infra'}

# Prometheus storage config
openshift_prometheus_storage_access_modes=['ReadWriteOnce']
openshift_prometheus_storage_volume_name=prometheus
openshift_prometheus_storage_volume_size=10Gi
openshift_prometheus_storage_type='pvc'
openshift_prometheus_sc_name="glusterfs-storage"

# For prometheus-alertmanager
openshift_prometheus_alertmanager_storage_access_modes=['ReadWriteOnce']
openshift_prometheus_alertmanager_storage_volume_name=prometheus-alertmanager
openshift_prometheus_alertmanager_storage_volume_size=10Gi
openshift_prometheus_alertmanager_storage_type='pvc'
openshift_prometheus_alertmanager_sc_name="glusterfs-storage"

# For prometheus-alertbuffer
openshift_prometheus_alertbuffer_storage_access_modes=['ReadWriteOnce']
openshift_prometheus_alertbuffer_storage_volume_name=prometheus-alertbuffer
openshift_prometheus_alertbuffer_storage_volume_size=10Gi
openshift_prometheus_alertbuffer_storage_type='pvc'
openshift_prometheus_alertbuffer_sc_name="glusterfs-storage"

openshift_prometheus_node_exporter_image_version=v3.9.25

# Ansible Service Broker
openshift_hosted_etcd_storage_kind=dynamic
openshift_hosted_etcd_storage_volume_name=etcd-vol2
openshift_hosted_etcd_storage_access_modes=["ReadWriteOnce"]
openshift_hosted_etcd_storage_volume_size=10Gi
ansible_service_broker_local_registry_whitelist=['.*-apb$']

# If using Route53 or you're pointed to the master with a "vanity" name
openshift_master_public_api_url=https://ocp-poc.cloud.chx:8443
openshift_master_public_console_url=https://ocp-poc.cloud.chx:8443/console
openshift_master_cluster_public_hostname=ocp-poc.cloud.chx
openshift_master_api_port=8443
openshift_master_console_port=8443

# The following enabled htpasswd authentication
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/openshift-passwd'}]
openshift_master_htpasswd_users={'developer': '$apr1$q2fVVf46$85HP/4JHGYeFBKAKPBblo0'}

# OpenShift host groups

# host group for etcd
[etcd]
poc-master.cloud.chx

# host group for masters - set scedulable to "true" for the web-console pod
[masters]
poc-master.cloud.chx openshift_schedulable=true

# host group for nodes, includes region info
[nodes]
poc-master.cloud.chx openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
poc-node1.cloud.chx  openshift_node_labels="{'region': 'primary', 'zone': 'z1'}"
poc-node2.cloud.chx  openshift_node_labels="{'region': 'primary', 'zone': 'z2'}"
poc-node3.cloud.chx  openshift_node_labels="{'region': 'primary', 'zone': 'z3'}"

[glusterfs]
# "standalone" glusterfs nodes STILL need to be in the "[nodes]" section
poc-node1.cloud.chx glusterfs_ip=192.168.1.251 glusterfs_zone=1 glusterfs_devices='[ "/dev/vdc" ]'
poc-node2.cloud.chx glusterfs_ip=192.168.1.252 glusterfs_zone=2 glusterfs_devices='[ "/dev/vdc" ]'
poc-node3.cloud.chx glusterfs_ip=192.168.1.253 glusterfs_zone=3 glusterfs_devices='[ "/dev/vdc" ]'
##
##
